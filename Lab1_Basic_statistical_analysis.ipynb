{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxKzyR01ybYX"
   },
   "source": [
    "<center>\n",
    "    <img src=\"https://gitlab.com/ibm/skills-network/courses/placeholder101/-/raw/master/labs/module%201/images/IDSNlogo.png\" width=\"300\" alt=\"cognitiveclass.ai logo\"  />\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKJCVZr8y4BP"
   },
   "source": [
    "# **Data Science in Insurance. Basic statistical analysis**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLJhUfyJzGWN"
   },
   "source": [
    "Estaimted time needed: **45** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvG7RdB0hTZf"
   },
   "source": [
    "In this lab, you will learn how to calculate basic descriptive statistics using Apache Spark.\n",
    "You will work with insurance statistical data granted by Ukrainian government https://www.nfp.gov.ua/ua/Konsolidovani-zvitni-dani.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqg7oe2v4Omt"
   },
   "source": [
    "## Prerequirements\n",
    "\n",
    "*   Basic Python knowledge\n",
    "*   Basic Apache Spark knowledge\n",
    "*   Basic SQL knowledge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2SbEPNiEMv_"
   },
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3r4ecjG1ERvK"
   },
   "source": [
    "After completing this lab, you will be able to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGnIj7U2ESw5"
   },
   "source": [
    "*   Load statistical data from .csv file.\n",
    "*   Understand what insurance loss ratio is.\n",
    "*   Visualize data using matplotlib\n",
    "*   Calculate\n",
    "    *   Minimal and maximal value\n",
    "    *   Mean\n",
    "    *   Standard Deviation\n",
    "    *   Skew and Kurtousis\n",
    "    *   Correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c4GR5cXEfQS"
   },
   "source": [
    "## Importing libraries/Defining auxiliary functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FF-oG6h7hTZo"
   },
   "source": [
    "**Environment.** This notebook is designed to run in Skills Network Labs. Therefore, we install Apache Spark in local mode for test purposes only. Please don't use it in production.\n",
    "\n",
    "**Running outside Skills Network Labs.** This notebook was tested within Skills Network Labs. Running in another environment should work as well, but is not guaranteed and may require different setup routine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prXbLALdLa5X"
   },
   "outputs": [],
   "source": [
    "conda install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9mBmdZ7hTZv"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from pyspark import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession\n",
    "except ImportError as e:\n",
    "    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXiUtelr75D8"
   },
   "source": [
    "The entry point into all functionality in Spark is the SparkSession class. To create a basic SparkSession, we use SparkSession.builder:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdGO3tdwhTZw"
   },
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQJC1x5oNz6k"
   },
   "source": [
    "## LOADING DATA FROM .CSV FILE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhF98WOJehXi"
   },
   "source": [
    "Let's go and get our dataset file from the repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6ye7ChMecIr"
   },
   "outputs": [],
   "source": [
    "!wget https://author.skills.network/quicklabs/Data%20Science%20in%20Insurance.%20Basic%20statistical%20analysis.csv UAInsurance.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKiNlZ5GeZBq"
   },
   "source": [
    "Now we need to transform our data into a dataframe. Here is a small example of how to construct a Spark dataframe from .csv file. Our .csv does not contain information on the type of the data, but we can use schema to construct the dataset correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJMJgNy9i5rK"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([\\\n",
    "                      StructField(\"Year\",IntegerType()),\\ \n",
    "                      StructField(\"Quarter\",StringType()),\\\n",
    "                      StructField(\"Premiums\",DoubleType()),\\\n",
    "                      StructField(\"Claims \",DoubleType()),\\\n",
    "                      StructField(\"Loss\",DoubleType()),\\\n",
    "                      ])\n",
    "df = (spark.read.format(\"csv\").options(header=\"true\").schema(schema).load('UAInsurance.csv'))\n",
    "df.createOrReplaceTempView('insurance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QpNMqkyM6AT"
   },
   "source": [
    "Now let's look at our dataset. It contains aggergated per quarter statistics for insurance claims and premiums of Ukrainian insurance companies provided by Ukrainian Finantial Regulation Commeetee. \"Loss\" field is the corresponding insurance loss ratio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHuh-hTpNHv3"
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSNgZF36_V08"
   },
   "source": [
    "Now we have a Spark Dataframe and can handle it using SparkSQL like a database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G24D6BI4v1Yb"
   },
   "source": [
    "## Plotting data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qrzw0b0GxawB"
   },
   "source": [
    "Now let's visualize our data.\n",
    "We will use Matplotlib library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dak90EAZyaZR"
   },
   "source": [
    "Importing the library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pVsZjoocv7q8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clhnzf1kzwQ_"
   },
   "source": [
    "We need to collect the data from the Spark dataframe first and convert it into a Numpy array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OdPauBiZzs6k"
   },
   "outputs": [],
   "source": [
    "date=np.array(spark.sql('SELECT Quarter FROM insurance').collect())[:,0]\n",
    "loss=np.array(spark.sql('SELECT Loss FROM insurance').collect())[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_b9Stpt8yfOQ"
   },
   "source": [
    "Plotting insurance loss ratio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LOutwFNYykZB"
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,4)\n",
    "plt.plot(date,loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oAeivp_mmoU"
   },
   "source": [
    "## INSURANCE LOSS RATIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9WUwBEYmqOS"
   },
   "source": [
    "For insurance, the loss ratio is the ratio of total losses incurred (paid and reserved) in claims plus adjustment expenses divided by the total premiums earned. For example, if an insurance company pays 60 USD in claims for every 100 USD in collected premiums, then its loss ratio is 60% with a profit ratio/gross margin of 40% or 40 USD.\n",
    "Loss ratios typically range from 40% - 60%  for property and casualty insurance to more then 80% for medical insurance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaL26MDnhTZx"
   },
   "source": [
    "**Hint:** All functions can be implemented using DataFrames, ApacheSparkSQL or RDDs. We are only interested in the result. You are given the reference to the data frame in the \"df\" parameter and in case you want to use SQL just use the \"spark\" parameter which is a reference to the global SparkSession object. Finally if you want to use RDDs just use \"df.rdd\" for obtaining a reference to the underlying RDD object. But we discourage using RDD at this point in time.\n",
    "\n",
    "We prepared a small dataset with insurance statistical data granted by Ukrainian govenment https://www.nfp.gov.ua/ua/Konsolidovani-zvitni-dani.html, so lets analyze insurance loss ratio for Ukraine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PywnNORM8IrF"
   },
   "source": [
    "**TODO:** There are seven functions you have to implement but it is ok to pass 4 of them.\n",
    "\n",
    "**Gentle reminder:** Please also make sure that you hit the play button on the corresponding cell again on each change of a function to make it available to the rest of this notebook.\n",
    "\n",
    "Let's start! Just make sure you hit the play button on each cell from top to bottom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sarupEHd8NCD"
   },
   "source": [
    "## FUNCTION 1. Minimal insurance loss.\n",
    "\n",
    "Please calculate the minimal global insurance loss ratio for Ukraine during 2012-2019 years. We've provided a little skeleton for you in case you want to use SQL. Everything can be implemented using SQL only if you like.\n",
    "\n",
    "Change ##INSERT YOUR CODE HERE## with your code and hit the play button on the left side of the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lsNon7fshTZx"
   },
   "outputs": [],
   "source": [
    "def minLoss():\n",
    "    #TODO Please enter your code here, you are not required to use the template code below\n",
    "    return spark.sql(\"SELECT ##INSERT YOUR CODE HERE##(Loss) as minloss from insurance\").first().minloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsjXLFbAoG4P"
   },
   "source": [
    "Lets test our function. If you've done everything right, you will see the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdmbyhOboE5U"
   },
   "outputs": [],
   "source": [
    "print(\"Minimal insurance loss ratio for Ukraine in 2012-2019yy is\", minLoss()*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TX4c6nkTEo3I"
   },
   "source": [
    "<details><summary>Double-click <b>here</b> for the solution</summary> \n",
    "def correlationLoss():\n",
    "    return spark.sql(\"SELECT min(Loss) as minloss from insurance\").first().minloss\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UcXaxXVNhTZy"
   },
   "source": [
    "## FUNCTION 2. Mean.\n",
    "\n",
    "Now calculate the mean value of the loss ratio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLHoBH3vhTZz"
   },
   "outputs": [],
   "source": [
    "def meanLoss():\n",
    "    #TODO Please enter your code here, you are not required to use the template code below\n",
    "    return spark.sql(\"SELECT ##INSERT YOUR CODE HERE##(Loss) as meantemp from insurance\").first().meantemp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_QRJ3dJqk1T"
   },
   "source": [
    "Let's test our function. If you've done everything right, you will see the result. Keeping in mind that the insurance loss ratio is premiums/claims and it is usually between 40-60%, do you think that Ukrainian insurance companies pay their clients enough?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBeO7n_GqX3y"
   },
   "outputs": [],
   "source": [
    "print(\"Mean insurance loss ratio for Ukraine in 2012-2019yy is\", meanLoss()*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYDPhQdhKxa2"
   },
   "source": [
    "<details><summary>Double-click <b>here</b> for the solution</summary> \n",
    "\n",
    "```\n",
    "return spark.sql(\"SELECT mean(Loss) as minloss from insurance\").first().minloss\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsDd-4QwhTZz"
   },
   "source": [
    "## FUNCTION 3. Max.\n",
    "\n",
    "Please do the same for the maximum of the loss ratio now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WoSpNmvlhTZ0"
   },
   "outputs": [],
   "source": [
    "def maxLoss():\n",
    "    #TODO Please enter your code here, you are not required to use the template code below\n",
    "    return spark.sql(\"SELECT ##INSERT YOUR CODE HERE##(Loss) as maxloss from insurance\").first().maxloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98x-jV0RsiUq"
   },
   "source": [
    "Let's test our function. If you've done everything right, you will see the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtreMeOwsdXS"
   },
   "outputs": [],
   "source": [
    "print(\"Maximal insurance loss ratio for Ukraine in 2012-2019yy is\", maxLoss()*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDyOzgJQK78X"
   },
   "source": [
    "<details><summary>Double-click <b>here</b> for the solution</summary>\n",
    "\n",
    "```\n",
    "return spark.sql(\"SELECT max(Loss) as maxloss from insurance\").first().maxloss\n",
    "```\n",
    "\n",
    "</details> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8pLfFezhTZ1"
   },
   "source": [
    "## FUNCTION 4. Standard deviation.\n",
    "\n",
    "Please do the same for the standard deviation now.\n",
    "\n",
    "The standard deviation is a measure of variation or dispersion amount of a set of values. A low standard deviation indicates that the values tend to be close to the mean, while a high standard deviation indicates that the values are spread out over a wider range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LveHVw2fhTZ2"
   },
   "outputs": [],
   "source": [
    "def sdLoss():\n",
    "    #TODO Please enter your code here, you are not required to use the template code below\n",
    "    return spark.sql(\"SELECT ##INSERT YOUR CODE HERE##_pop(Loss) as sdloss from insurance\").first().sdloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIWR7ZFDsxqp"
   },
   "source": [
    "Let's test our function. If you've done everything right, you will see the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Be_ubQ3Ds3ra"
   },
   "outputs": [],
   "source": [
    "print(\"Standard deviation of insurance loss ratio for Ukraine in 2012-2019yy is\", sdLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pR-T1xXtGti"
   },
   "source": [
    "<details><summary>Double-click <b>here</b> for the solution</summary>\n",
    "\n",
    "```\n",
    "return spark.sql(\"SELECT stdev_pop(Loss) as sdloss from insurance\").first().sdloss\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbSVdoIahTZ2"
   },
   "source": [
    "## FUNCTION 5. Skewness.\n",
    "\n",
    "Please do the same for the skewness now.\n",
    "\n",
    "Skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive, zero, negative, or undefined.\n",
    "\n",
    "**Guidelines:** Since the SQL statement for this is a bit more complicated, we've provided a skeleton for you. You have to insert your custom code at four positions in order to make the function work. Alternatively, you can also remove everything and implement it on your own. Note that we are making use of the two previously defined functions, so please make sure they are correct. Also note that we are making use of Python's string formatting capabilities where the results of the two function calls to \"meanLoss\" and \"sdLoss\" are inserted at the \"%s\" symbols in the SQL string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xc4sbtUrhTZ2"
   },
   "outputs": [],
   "source": [
    "def skewLoss():    \n",
    "    return spark.sql(\"\"\"\n",
    "SELECT \n",
    "    (\n",
    "        1/##INSERT YOUR CODE HERE##\n",
    "    ) *\n",
    "    SUM (\n",
    "        POWER(##INSERT YOUR CODE HERE##-%s,3)/POWER(%s,3)\n",
    "    )\n",
    "\n",
    "as skloss from insurance\n",
    "                    \"\"\" %(meanLoss(),sdLoss())).first().skloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYKBLfbPt3VR"
   },
   "source": [
    "Let's test our function. If you've done everything right, you will see the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9x6XhYEjt0I9"
   },
   "outputs": [],
   "source": [
    "print(\"Skewness  of insurance loss ratio for Ukraine in 2012-2019yy is\", skewLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUQloNBhgIvE"
   },
   "source": [
    "<details><summary>Double-click <b>here</b> for the solution</summary>\n",
    "\n",
    "```\n",
    "def skewLoss():    \n",
    "    return spark.sql(\"\"\"\n",
    "SELECT \n",
    "    (\n",
    "        1/COUNT(Loss)\n",
    "    ) *\n",
    "    SUM (\n",
    "        POWER(Loss-%s,3)/POWER(%s,3)\n",
    "    )\n",
    "\n",
    "as skloss from insurance \n",
    "                    \"\"\" %(meanLoss(),sdLoss(),)).\n",
    "                    first().skloss\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGHG-AkthTZ3"
   },
   "source": [
    "## FUNCTION 6. Kurtosis.\n",
    "\n",
    "The standard measure of a distribution's kurtosis, originating with Karl Pearson, is a scaled version of the fourth moment of the distribution. This number is related to the tails of the distribution: a higher kurtosis indicates greater extremity of deviations (or outliers), and not the configuration of data near the mean.\n",
    "\n",
    "Hence kurtosis is the 4th statistical moment, you can make use of the code for skew which is the 3rd statistical moment. Actually only two things are different.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdtnqazghTZ3"
   },
   "outputs": [],
   "source": [
    "def kurtosisLoss():    \n",
    "        return spark.sql(\"\"\"\n",
    "SELECT \n",
    "    (\n",
    "        1/##INSERT YOUR CODE HERE##\n",
    "    ) *\n",
    "    SUM (\n",
    "        POWER(##INSERT YOUR CODE HERE##-%s,4)/POWER(%s,4)\n",
    "    )\n",
    "as kloss from insurance\n",
    "                    \"\"\" %(meanLoss(),sdLoss())).first().kloss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sb0L1tNouArx"
   },
   "source": [
    "Let's test our function. If you've done everything right, you will see the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_rb7SN6uBny"
   },
   "outputs": [],
   "source": [
    "print(\"Kurtosis of insurance loss ratio for Ukraine in 2012-2019yy is\", kurtosisLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDszAEbNhfCC"
   },
   "source": [
    "<details><summary>Double-click <b>here</b> for the solution</summary>\n",
    "\n",
    "```\n",
    "def kurtosisLoss():    \n",
    "        return spark.sql(\"\"\"\n",
    "SELECT \n",
    "    (\n",
    "        1/COUNT(Loss)\n",
    "    ) *\n",
    "    SUM (\n",
    "        POWER(Loss-%s,4)/POWER(%s,4)\n",
    "    )\n",
    "as kloss from insurance\n",
    "                    \"\"\" %(meanLoss(),sdLoss())).first().kloss\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-LFInxWhTZ3"
   },
   "source": [
    "## FUNCTION 7. Correlation.\n",
    "\n",
    "Now lets find the correlation between premiums and claims.\n",
    "\n",
    "The correlation coefficient ranges from -1 to 1 and shows us how pieces of data are related to each other. If the value is near 1, then it is said to be a perfect correlation: as one variable (value of premiums in our case) increases, the other (corresponding value of claims) tends to increase too. In the case of -1 the dependence is inverse: if one value increases, the other tends to decrease.\n",
    "\n",
    "**Hint:** This can be solved easily using SQL as well but, as shown in the lecture, also using RDDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05FEvgiZhTZ4"
   },
   "outputs": [],
   "source": [
    "def correlationLoss():\n",
    "    #TODO Please enter your code here, you are not required to use the template code below\n",
    "    return spark.sql(\"SELECT ##INSERT YOUR CODE HERE##(premiums, claims ) as corr from insurance\").first().corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-O7KTVUucST"
   },
   "source": [
    "Let's test our function. If you've done everything right, you will see how the amount of claims paid correlates with premiums (income) of Ukrainian insurance companies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-qd3dtiu0G5"
   },
   "outputs": [],
   "source": [
    "print(\"Correlation between premiums and claims of Ukrainian insurance companies in 2012-2019yy is\", correlationLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVNHXBPEiNXF"
   },
   "source": [
    "<details><summary>Double-click <b>here</b> for the solution</summary>\n",
    "\n",
    "```\n",
    "def correlationLoss():\n",
    "    return spark.sql(\"SELECT corr(premiums, claims ) as corr from insurance\").first().corr  \n",
    "\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DGzH0DFDC2R"
   },
   "source": [
    "## Assignment submission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2WFdJEmhTZ_"
   },
   "source": [
    "Congratulations, you are ready to submit the notebook to the grader.\n",
    "The first thing we need to do is to install a little helper library for submitting the solutions to the Coursera grader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMNYhSuLhTZ_"
   },
   "outputs": [],
   "source": [
    "!rm -f rklib.py\n",
    "!wget https://raw.githubusercontent.com/IBM/coursera/master/rklib.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebU-Q5v1hTaA"
   },
   "source": [
    "Now it’s time to submit your solution. Please provide your email address (in the line \"email = \") and obtained token on the grader’s submission page in Coursera (in the line \"token = \"), then execute the cell.\n",
    "\n",
    "**HOWTO provide email:** you should input your email address and your personalized token in the corresponding lines in the quotes like this: email = \"XXXX@mail.com\". You can input them instead of or before \"###*YOUR_EMAIL_GOES_HERE*###\".\n",
    "\n",
    "**Where can I get my token:** a personalized token can be obtained on a designated page of the courses assignment, on a bookmark \"Instructions\", by clicking a link \"Generate new token\" located on the right in \"How to submit\" section.\n",
    "\n",
    "**Hint:** take a look here if you need more information https://youtu.be/GcDo0Rwe06U?t=276\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWKJY_lThTaA"
   },
   "outputs": [],
   "source": [
    "from rklib import submitAll\n",
    "import json\n",
    "key = \"Suy4biHNEeimFQ479R3GjA\"\n",
    "email = ###_YOUR_EMAIL_GOES_HERE_###\n",
    "token = ###_YOUR_TOKEN_GOES_HERE_###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUQx7TmzhTaB"
   },
   "outputs": [],
   "source": [
    "parts_data = {}\n",
    "parts_data[\"FWMEL\"] = json.dumps(min_loss)\n",
    "parts_data[\"3n3TK\"] = json.dumps(mean_loss)\n",
    "parts_data[\"KD3By\"] = json.dumps(max_loss)\n",
    "parts_data[\"06Zie\"] = json.dumps(sd_loss)\n",
    "parts_data[\"Qc8bI\"] = json.dumps(skew_loss)\n",
    "parts_data[\"LoqQi\"] = json.dumps(kurtosis_loss)\n",
    "parts_data[\"ehNGV\"] = json.dumps(correlation_loss)\n",
    "submitAll(email, token, key, parts_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEy_2X8PGAi5"
   },
   "source": [
    "## References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xo0ejtseGEiR"
   },
   "source": [
    "1.  **Loading .csv data with Pyspark** https://towardsdatascience.com/pyspark-import-any-data-f2856cda45fd\n",
    "\n",
    "2.  **Matplotlib documentation** https://matplotlib.org/stable/tutorials/introductory/pyplot.html\n",
    "\n",
    "3.  **Insurance Loss ratio** https://en.wikipedia.org/wiki/Loss_ratio#Insurance_Loss_Ratio\n",
    "\n",
    "4.  **Apache Spark DataFrame:** https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "\n",
    "5.  **Mean** https://en.wikipedia.org/wiki/Arithmetic_mean\n",
    "\n",
    "6.  **Standard deviation** https://en.wikipedia.org/wiki/Standard_deviation\n",
    "\n",
    "7.  **Skewness** https://en.wikipedia.org/wiki/Skewness\n",
    "\n",
    "8.  **Kurtosis** https://en.wikipedia.org/wiki/Kurtosis\n",
    "\n",
    "9.  **Pearsons correlation coefficient** https://www.statisticssolutions.com/pearsons-correlation-coefficient/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uN5VtW28StyY"
   },
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nH9LWOE9Sxke"
   },
   "source": [
    "[Bogdan Norkin](https://www.researchgate.net/profile/Bogdan-Norkin?utm_medium=Exinfluencer\\&utm_source=Exinfluencer\\&utm_content=000026UJ\\&utm_term=10006555\\&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsdatascienceininsurancebasicstatisticalanalysis25199191-2021-01-01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXcqI3Y-S00w"
   },
   "source": [
    "Copyright © 2021 IBM Corporation. This notebook and its source code are released under the terms of the [MIT License](https://cognitiveclass.ai/mit-license/?utm_medium=Exinfluencer\\&utm_source=Exinfluencer\\&utm_content=000026UJ\\&utm_term=10006555\\&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkQuickLabsdatascienceininsurancebasicstatisticalanalysis25199191-2021-01-01).\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab1_Basic_statistical_analysis.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
